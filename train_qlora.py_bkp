import os
import torch
import torch.nn as nn
import torch.optim as optim
# Use torch.amp directly instead of torch.cuda.amp
from torch.amp import autocast, GradScaler # Updated import
import argparse
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import wandb
import numpy as np
from dataloader import create_dataloaders

# Import both the model class and CONFIG from train_projection
# Make sure train_projection.py is accessible and correct
try:
    from train_projection import SigLIPProjectionModel, CONFIG as PROJECTION_CONFIG
except ImportError as e:
    print(f"Error importing from train_projection: {e}")
    print("Please ensure train_projection.py is in the Python path and has SigLIPProjectionModel and CONFIG.")
    # Provide dummy values to allow script loading for inspection, but it won't run.
    PROJECTION_CONFIG = {"PROJECTION_DIM": 2048, "PROJECTION_LAYERS": 2}
    class SigLIPProjectionModel(nn.Module):
        def __init__(self, *args, **kwargs): super().__init__(); print("Using Dummy SigLIPProjectionModel")
        def forward(self, x): return x # Dummy forward

# Configuration
CONFIG = {
    # Model parameters
    "SIGLIP_MODEL": "google/siglip-so400m-patch14-384",
    "PHI_MODEL": "microsoft/phi-3-mini-4k-instruct",
    "PROJECTION_MODEL_PATH": "siglip_phi3_projection/best_model.pt", # Make sure this path is correct

    # Use the same projection parameters from train_projection.py
    "PROJECTION_DIM": PROJECTION_CONFIG["PROJECTION_DIM"],
    "PROJECTION_LAYERS": PROJECTION_CONFIG["PROJECTION_LAYERS"],

    # QLoRA parameters
    "LORA_R": 16,
    "LORA_ALPHA": 32,
    "LORA_DROPOUT": 0.05,
    "LORA_TARGET_MODULES": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],

    # Quantization parameters
    "QUANTIZATION_BITS": 4,  # 4-bit quantization
    "QUANT_TYPE": "nf4",     # NF4 data type

    # Training parameters
    "BATCH_SIZE": 8, # Adjust based on GPU memory
    "LEARNING_RATE": 2e-4, # AdamW default is 1e-3, common for LoRA is 1e-4 to 3e-4
    "WEIGHT_DECAY": 0.01,
    "EPOCHS": 1,
    "SAVE_EVERY": 1,
    "GRADIENT_ACCUMULATION_STEPS": 4, # Effective batch size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS

    # Optimizer parameters
    "OPTIMIZER": "adamw",
    "SCHEDULER": "cosine",
    "LR_MIN_FACTOR": 0.1,   # Minimum LR will be LR * this factor
    "WARMUP_RATIO": 0.03,   # Percentage of steps for warmup

    # Mixed precision
    "USE_MIXED_PRECISION": True,
    "COMPUTE_DTYPE": torch.bfloat16, # Define compute dtype centrally (bfloat16 recommended for Ampere+)

    # Data parameters
    "DATA_DIR": "cifar10_vlm_dataset", # Make sure this path is correct
    "NUM_WORKERS": 4,

    # Output parameters
    "OUTPUT_DIR": "phi3_qlora_adapter",
    "USE_WANDB": False,
}

class MultimodalPhiModel(nn.Module):
    """Model that combines SigLIP projection with Phi-3 for multimodal capabilities."""

    def __init__(self, siglip_model_name, phi_model_name, projection_model_path,
                 quantization_config=None, projection_dim=None, projection_layers=None,
                 compute_dtype=torch.bfloat16): # Added compute_dtype argument
        """
        Initialize the multimodal model.

        Args:
            siglip_model_name (str): Name of the SigLIP model
            phi_model_name (str): Name of the Phi-3 model
            projection_model_path (str): Path to the trained projection model
            quantization_config: Configuration for quantization
            projection_dim: Dimension of the projection layer
            projection_layers: Number of layers in projection MLP
            compute_dtype: Data type for computation (e.g., torch.bfloat16)
        """
        super().__init__()

        self.compute_dtype = compute_dtype
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load processor first to check dimensions if needed
        print(f"Loading processor for {phi_model_name}...")
        # Use trust_remote_code=True if required by the model, common for newer models like Phi-3
        self.processor = AutoProcessor.from_pretrained(phi_model_name, trust_remote_code=True)

        # --- Start Fix for Pad Token ---
        # Access pad_token directly on the processor object
        if not hasattr(self.processor, 'pad_token') or self.processor.pad_token is None:
             print("Processor does not have pad_token set.")
             # Check if eos_token exists and is suitable
             if hasattr(self.processor, 'eos_token') and self.processor.eos_token:
                 print(f"Setting pad_token to eos_token: '{self.processor.eos_token}'")
                 self.processor.pad_token = self.processor.eos_token
             else:
                 # Fallback: Add a generic pad token if EOS is also missing (unlikely but safe)
                 print("Warning: EOS token also missing. Adding a default <|pad|> token.")
                 self.processor.add_special_tokens({'pad_token': '<|pad|>'})
        else:
            print(f"Processor pad_token already set: '{self.processor.pad_token}'")
        # --- End Fix for Pad Token ---


        # Load SigLIP projection model
        # Ensure it's loaded with the compute_dtype if possible, or cast later
        # Note: SigLIPProjectionModel itself might need modification to accept dtype
        # For now, we'll cast after loading.
        print(f"Initializing SigLIPProjectionModel (SigLIP: {siglip_model_name}, Phi: {phi_model_name})...")
        self.projection_model = SigLIPProjectionModel(
            siglip_model_name=siglip_model_name,
            phi_model_name=phi_model_name, # Pass Phi name for potential dim checking inside projection model
            projection_dim=projection_dim,
            num_layers=projection_layers
        )

        # Load trained projection weights
        if projection_model_path and os.path.exists(projection_model_path):
            print(f"Loading projection model weights from {projection_model_path}")
            try:
                checkpoint = torch.load(projection_model_path, map_location="cpu")
                # Load state dict before moving to device/dtype
                self.projection_model.projection.load_state_dict(checkpoint["model_state_dict"])
                print("Successfully loaded projection weights.")
            except Exception as e:
                 print(f"Error loading projection weights from {projection_model_path}: {e}")
                 print("Warning: Using untrained projection layer.")

        else:
            print(f"Warning: Projection model path '{projection_model_path}' not found or not specified. Using untrained projection.")

        # Move the projection model to the correct device and data type
        print(f"Casting projection model to {self.compute_dtype} on device {self.device}")
        self.projection_model.to(self.device, dtype=self.compute_dtype)

        # Freeze the projection model
        print("Freezing projection model parameters.")
        for param in self.projection_model.parameters():
            param.requires_grad = False
        self.projection_model.eval() # Also set to evaluation mode

        # Load Phi-3 model with quantization for QLoRA
        # Use trust_remote_code=True here as well
        model_load_kwargs = {
            "torch_dtype": self.compute_dtype,
            "device_map": "auto", # Handles device placement for large models
            "trust_remote_code": True
        }
        if quantization_config:
            print("Loading Phi-3 model with quantization...")
            model_load_kwargs["quantization_config"] = quantization_config
        else:
            print("Loading Phi-3 model without quantization...")

        try:
            self.phi = AutoModelForCausalLM.from_pretrained(phi_model_name, **model_load_kwargs)
            print("Phi-3 model loaded.")
        except Exception as e:
            print(f"Error loading Phi-3 model '{phi_model_name}': {e}")
            # Example: Handle potential trust_remote_code issues or connection errors
            if "trust_remote_code=True" in str(e):
                print("Loading failed. Ensure you trust the model code or check network connection.")
            raise # Re-raise the exception to halt execution if model loading fails

        # --- Set Model's Pad Token ID ---
        # Ensure the model's config uses the same pad token ID as the processor
        # Access pad_token_id directly on the processor
        if hasattr(self.processor, 'pad_token_id') and self.processor.pad_token_id is not None:
             print(f"Setting model's pad_token_id to: {self.processor.pad_token_id}")
             self.phi.config.pad_token_id = self.processor.pad_token_id
        else:
             print("Warning: Could not determine processor's pad_token_id. Model config might be inconsistent.")
        # ---

        # Prepare model for k-bit training if quantized
        if quantization_config:
            print("Preparing Phi-3 model for k-bit training...")
            self.phi = prepare_model_for_kbit_training(self.phi)


    def forward(self, images, input_ids, attention_mask, labels=None):
        """
        Forward pass through the model.

        Args:
            images (torch.Tensor): Batch of images (should be on the correct device)
            input_ids (torch.Tensor): Input token IDs (should be on the correct device)
            attention_mask (torch.Tensor): Attention mask (should be on the correct device)
            labels (torch.Tensor, optional): Labels for computing loss (should be on the correct device)

        Returns:
            torch.Tensor: Model output (contains loss if labels provided)
        """
        # Ensure images are on the same device and dtype as the projection model expects
        images = images.to(self.projection_model.siglip.device, dtype=self.compute_dtype)

        # Get projected image embeddings from the frozen projection model
        # No need for torch.no_grad() as the model is frozen and in eval mode
        image_embeddings = self.projection_model(images) # Shape: [batch_size, proj_dim]

        # Ensure projected embeddings match Phi's expected embedding dimension
        phi_embedding_dim = self.phi.get_input_embeddings().weight.shape[-1]
        if image_embeddings.shape[-1] != phi_embedding_dim:
             raise ValueError(f"Projected image embedding dimension ({image_embeddings.shape[-1]}) "
                              f"does not match Phi-3 embedding dimension ({phi_embedding_dim}). "
                              "Check your SigLIPProjectionModel's output dimension (projection_dim).")

        # Get the token embeddings from Phi model
        # Ensure input_ids are on the correct device (handled by device_map="auto" usually)
        # input_ids = input_ids.to(self.phi.device) # Usually not needed with device_map
        input_embeddings = self.phi.get_input_embeddings()(input_ids) # Shape: [batch_size, seq_len, phi_emb_dim]

        # Prepare image embeddings for concatenation: ensure same device and dtype as input_embeddings
        image_embeddings = image_embeddings.to(input_embeddings.device, dtype=input_embeddings.dtype)

        # Add the sequence length dimension: [batch_size, 1, phi_emb_dim]
        image_embeddings_unsqueezed = image_embeddings.unsqueeze(1)

        # Concatenate image embedding at the beginning
        # Input Embeddings: [ImageEmb, TokenEmb_1, TokenEmb_2, ...]
        final_embeddings = torch.cat(
            [image_embeddings_unsqueezed, input_embeddings[:, 1:, :]], # Exclude the original first token embedding
            dim=1
        )

        # Adjust attention mask: The image embedding corresponds to the first position.
        # The original attention mask should be kept as is if it correctly masks padding.
        # The image token should be attended to (mask=1).
        # Make sure attention mask is on the correct device
        # attention_mask = attention_mask.to(self.phi.device) # Usually not needed with device_map

        # Ensure labels are also on the correct device if provided
        if labels is not None:
            # labels = labels.to(self.phi.device) # Usually not needed with device_map
            pass # device_map should handle label device placement relative to logits

        # Forward pass through Phi model using the combined embeddings
        outputs = self.phi(
            inputs_embeds=final_embeddings,
            attention_mask=attention_mask, # Use original mask, assuming image replaces a non-padded token
            labels=labels,
            return_dict=True
        )

        return outputs

def train_epoch(model, train_loader, optimizer, scaler, device, gradient_accumulation_steps=1, compute_dtype=torch.bfloat16): # Added compute_dtype
    """Train for one epoch."""
    model.train() # Set the main model (PEFT-adapted Phi) to train mode
    model.projection_model.eval() # Keep projection model frozen and in eval mode
    total_loss = 0.0
    num_samples = 0

    progress_bar = tqdm(train_loader, desc="Training", leave=False)
    optimizer.zero_grad() # Zero gradients initially and after each optimizer step

    for step, batch in enumerate(progress_bar):
        # Move data to the primary device (CPU or CUDA:0 generally)
        # device_map in PhiModel will handle moving parts to other GPUs if needed
        # The projection model expects data on its device (set during init)
        images = batch["image"] # Keep on CPU initially, move inside forward pass
        input_ids = batch["input_ids"].to(device) # Move inputs to primary device
        attention_mask = batch["attention_mask"].to(device) # Move inputs to primary device

        # Create labels for causal language modeling
        # Labels need to end up on the same device as the logits output by self.phi
        labels = input_ids.clone() # Clone first

        # --- Label Masking ---
        # Image embedding is at index 0. We predict token originally at index 1 using the image embedding.
        # The first label corresponds to the prediction for the token at index 1.
        # So, we mask the loss calculation for the position corresponding to the input image embedding (index 0).
        labels[:, 0] = -100 # Ignore loss for the first position (image)

        # Ensure labels are moved to the device where the loss will be computed
        labels = labels.to(device) # Move labels to primary device initially

        # Forward pass with mixed precision context manager
        # autocast targets CUDA device by default if available
        with autocast(device_type=device.type, dtype=compute_dtype, enabled=scaler.is_enabled()):
             outputs = model(
                 images=images, # Will be moved to projection_model.device inside forward
                 input_ids=input_ids,
                 attention_mask=attention_mask,
                 labels=labels # Will be moved to phi.device inside forward if needed
             )
             # Loss is computed based on logits and labels on potentially different devices (due to device_map)
             # The loss value itself should be a scalar tensor on the primary device (or CPU)
             loss = outputs.loss

             # Normalize loss for accumulation
             if loss is not None: # Check if loss computation was successful
                 loss = loss / gradient_accumulation_steps
             else:
                 print("Warning: Loss is None. Skipping step.")
                 continue # Skip backpropagation if loss is None

        # Backward pass: scale loss and calculate gradients
        # scaler manages enabling/disabling based on its 'enabled' flag
        scaler.scale(loss).backward()

        # Update weights and zero gradients after accumulating enough steps
        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):
            # Unscale gradients before clipping
            scaler.unscale_(optimizer)
            # Clip gradients for trainable parameters
            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.phi.parameters()), 1.0)
            # Optimizer step - updates trainable parameters
            scaler.step(optimizer)
            # Update the scaler for the next iteration
            scaler.update()
            # Zero gradients for the next accumulation cycle
            optimizer.zero_grad()

        # Logging and tracking
        if loss is not None:
            batch_loss = loss.item() * gradient_accumulation_steps # Get the un-normalized loss for logging
            total_loss += batch_loss * images.size(0) # Accumulate total loss weighted by batch size
            num_samples += images.size(0)
            progress_bar.set_postfix({"loss": f"{batch_loss:.4f}"}) # Log loss for the current step


    # Return average loss per sample for the epoch
    avg_epoch_loss = total_loss / num_samples if num_samples > 0 else 0.0
    return avg_epoch_loss

def validate(model, val_loader, device, compute_dtype=torch.bfloat16): # Added compute_dtype
    """Validate the model."""
    model.eval() # Set the entire model (including Phi and projection) to evaluation mode
    total_loss = 0.0
    num_samples = 0

    with torch.no_grad(): # Disable gradient calculation for validation
        for batch in tqdm(val_loader, desc="Validation", leave=False):
            # Move data to device
            images = batch["image"] # Keep on CPU initially
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            # Create labels
            labels = input_ids.clone()
            labels[:, 0] = -100  # Mask the image token position
            labels = labels.to(device) # Move labels initially

            # Forward pass - use autocast for consistency if used in training
            with autocast(device_type=device.type, dtype=compute_dtype, enabled=CONFIG["USE_MIXED_PRECISION"]): # Use config flag directly
                outputs = model(
                    images=images,
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

            if outputs.loss is not None:
                total_loss += outputs.loss.item() * images.size(0) # Accumulate loss weighted by batch size
                num_samples += images.size(0)


    # Return average loss per sample for the validation set
    avg_val_loss = total_loss / num_samples if num_samples > 0 else 0.0
    return avg_val_loss

def main(args):
    # Print configuration
    print("--- Configuration ---")
    for key, value in vars(args).items():
        print(f"  {key}: {value}")
    print("---------------------")

    # Set up device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using primary device: {device}")

    # Determine compute dtype from config
    compute_dtype = CONFIG["COMPUTE_DTYPE"]
    print(f"Using compute dtype: {compute_dtype}")

    # Initialize wandb if enabled
    if args.use_wandb:
        try:
            wandb.init(
                project="phi3-qlora-multimodal", # Change project name if needed
                config=vars(args) # Log hyperparameters
            )
            print("Weights & Biases initialized.")
        except ImportError:
            print("Warning: wandb library not installed. Logging disabled.")
            args.use_wandb = False


    # Create dataloaders
    print("Creating dataloaders...")
    try:
        train_loader, val_loader = create_dataloaders(
            data_dir=args.data_dir,
            phi_processor_name=args.phi_model_name, # Pass processor name for tokenizer setup in dataloader
            batch_size=args.batch_size,
            num_workers=args.num_workers
        )
        print(f"Dataloaders created: {len(train_loader)} training batches ({len(train_loader.dataset)} samples), "
              f"{len(val_loader)} validation batches ({len(val_loader.dataset)} samples)")
    except Exception as e:
        print(f"Error creating dataloaders from '{args.data_dir}': {e}")
        print("Please ensure the dataset directory is correct and the dataloader script works.")
        return # Exit if dataloaders can't be created

    # Set up quantization config for QLoRA
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=compute_dtype, # Use defined compute_dtype
        bnb_4bit_quant_type=args.quant_type,
        bnb_4bit_use_double_quant=True # Common practice for QLoRA
    )
    print(f"Quantization config ({args.quant_type}, 4-bit) created.")

    # Create model
    print("Creating the MultimodalPhiModel...")
    try:
        model = MultimodalPhiModel(
            siglip_model_name=args.siglip_model_name,
            phi_model_name=args.phi_model_name,
            projection_model_path=args.projection_model_path,
            quantization_config=quantization_config,
            projection_dim=args.projection_dim,
            projection_layers=args.projection_layers,
            compute_dtype=compute_dtype # Pass compute_dtype
        )
    except Exception as e:
        print(f"Error initializing MultimodalPhiModel: {e}")
        return # Exit if model creation fails

    # Apply LoRA to the model
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        target_modules=args.lora_target_modules,
        lora_dropout=args.lora_dropout,
        bias="none", # Usually 'none' for LoRA
        task_type="CAUSAL_LM" # Important for PEFT
    )

    try:
        print("Applying LoRA adapter to the Phi-3 model...")
        model.phi = get_peft_model(model.phi, lora_config)
        print("LoRA adapter applied successfully.")
        model.phi.print_trainable_parameters()
    except Exception as e:
        print(f"Error applying LoRA adapter: {e}")
        print("Check LoraConfig target_modules and base model compatibility.")
        return

    # Set up optimizer - only optimize trainable LoRA parameters
    # Filter parameters that require gradients
    trainable_params = filter(lambda p: p.requires_grad, model.phi.parameters())
    optimizer = optim.AdamW(
        trainable_params,
        lr=args.learning_rate,
        weight_decay=args.weight_decay
    )
    print(f"Optimizer: AdamW with LR={args.learning_rate}, Weight Decay={args.weight_decay}")

    # Set up learning rate scheduler
    num_update_steps_per_epoch = len(train_loader) // args.gradient_accumulation_steps
    total_update_steps = num_update_steps_per_epoch * args.epochs # Total number of optimizer steps
    warmup_steps = int(total_update_steps * args.warmup_ratio)

    print(f"Total optimizer steps: {total_update_steps}, Warmup steps: {warmup_steps}")

    if args.scheduler.lower() == "cosine":
        # T_max should be the number of steps *after* warmup
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=max(1, total_update_steps - warmup_steps), # Ensure T_max is at least 1
            eta_min=args.learning_rate * args.lr_min_factor
        )
        print(f"Using CosineAnnealingLR scheduler with T_max={max(1, total_update_steps - warmup_steps)}, eta_min={args.learning_rate * args.lr_min_factor}")
    else: # Add other schedulers like 'linear' or default to cosine
         print(f"Warning: Scheduler '{args.scheduler}' not explicitly 'cosine'. Defaulting to CosineAnnealingLR.")
         scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=max(1, total_update_steps - warmup_steps),
            eta_min=args.learning_rate * args.lr_min_factor
         )
         # Add linear scheduler option if needed:
         # from transformers import get_linear_schedule_with_warmup
         # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_update_steps)

    # Set up gradient scaler for mixed precision
    # Use the primary device type ('cuda' or 'cpu')
    scaler = GradScaler(enabled=args.use_mixed_precision)
    print(f"Gradient Scaler enabled: {args.use_mixed_precision}")

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"Output directory: {args.output_dir}")

    # Training loop
    best_val_loss = float('inf')
    print(f"\n--- Starting Training for {args.epochs} Epochs ---")

    for epoch in range(args.epochs):
        print(f"\n--- Epoch {epoch+1}/{args.epochs} ---")

        # Train one epoch
        train_loss = train_epoch(
            model,
            train_loader,
            optimizer,
            scaler,
            device, # Primary device for data loading coordination
            gradient_accumulation_steps=args.gradient_accumulation_steps,
            compute_dtype=compute_dtype
        )

        # Validate
        val_loss = validate(
            model,
            val_loader,
            device, # Primary device
            compute_dtype=compute_dtype
        )

        # Step the scheduler *after* the epoch (CosineAnnealingLR steps per epoch based on T_max)
        # If using a step-based scheduler (like linear), step it inside the train_epoch loop after optimizer.step()
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr'] # Get current LR after scheduler step

        # Log metrics
        print(f"Epoch {epoch+1} Summary: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6e}")
        if args.use_wandb:
            wandb.log({
                "epoch": epoch + 1,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "learning_rate": current_lr
            }, step=epoch+1) # Log per epoch

        # Save best model adapter based on validation loss
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_path = os.path.join(args.output_dir, "best_adapter")
            print(f"New best validation loss: {val_loss:.4f}. Saving adapter to {save_path}...")
            model.phi.save_pretrained(save_path) # Saves only the adapter + config
            # Save the processor (contains tokenizer state) alongside the adapter
            model.processor.save_pretrained(save_path)
            print(f"Adapter saved.")


        # Save checkpoint adapter periodically
        if (epoch + 1) % args.save_every == 0 and (epoch + 1) < args.epochs: # Don't save checkpoint on last epoch if saving final model
            checkpoint_path = os.path.join(args.output_dir, f"adapter_checkpoint_epoch_{epoch+1}")
            print(f"Saving checkpoint adapter for epoch {epoch+1} to {checkpoint_path}...")
            model.phi.save_pretrained(checkpoint_path)
            model.processor.save_pretrained(checkpoint_path)
            print(f"Checkpoint saved.")

    # Save final model adapter
    final_path = os.path.join(args.output_dir, "final_adapter")
    print(f"Saving final adapter to {final_path}...")
    model.phi.save_pretrained(final_path)
    model.processor.save_pretrained(final_path)
    print("Final adapter saved.")

    print("\n--- Training Complete ---")
    print(f"Best validation loss achieved: {best_val_loss:.4f}")

    if args.use_wandb:
        wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="QLoRA fine-tuning for Phi-3 with SigLIP projection")

    # Get CONFIG from CONFIG dict
    compute_dtype = CONFIG["COMPUTE_DTYPE"]
    # Model arguments
    parser.add_argument("--siglip_model_name", type=str, default=CONFIG["SIGLIP_MODEL"])
    parser.add_argument("--phi_model_name", type=str, default=CONFIG["PHI_MODEL"])
    parser.add_argument("--projection_model_path", type=str, default=CONFIG["PROJECTION_MODEL_PATH"])

    # QLoRA arguments
    parser.add_argument("--lora_r", type=int, default=CONFIG["LORA_R"])
    parser.add_argument("--lora_alpha", type=int, default=CONFIG["LORA_ALPHA"])
    parser.add_argument("--lora_dropout", type=float, default=CONFIG["LORA_DROPOUT"])
    parser.add_argument("--lora_target_modules", type=str, nargs="+", default=CONFIG["LORA_TARGET_MODULES"])
    parser.add_argument("--quant_type", type=str, default=CONFIG["QUANT_TYPE"], choices=['nf4', 'fp4'])

    # Data arguments
    parser.add_argument("--data_dir", type=str, default=CONFIG["DATA_DIR"])
    parser.add_argument("--batch_size", type=int, default=CONFIG["BATCH_SIZE"])
    parser.add_argument("--num_workers", type=int, default=CONFIG["NUM_WORKERS"])

    # Training arguments
    parser.add_argument("--epochs", type=int, default=CONFIG["EPOCHS"])
    parser.add_argument("--learning_rate", type=float, default=CONFIG["LEARNING_RATE"])
    parser.add_argument("--weight_decay", type=float, default=CONFIG["WEIGHT_DECAY"])
    parser.add_argument("--save_every", type=int, default=CONFIG["SAVE_EVERY"])
    parser.add_argument("--optimizer", type=str, default=CONFIG["OPTIMIZER"])
    parser.add_argument("--scheduler", type=str, default=CONFIG["SCHEDULER"])
    parser.add_argument("--lr_min_factor", type=float, default=CONFIG["LR_MIN_FACTOR"])
    parser.add_argument("--warmup_ratio", type=float, default=CONFIG["WARMUP_RATIO"])
    # Use BooleanOptionalAction for flags: --use-mixed-precision or --no-use-mixed-precision
    parser.add_argument("--use_mixed_precision", action=argparse.BooleanOptionalAction, default=CONFIG["USE_MIXED_PRECISION"])
    parser.add_argument("--gradient_accumulation_steps", type=int, default=CONFIG["GRADIENT_ACCUMULATION_STEPS"])

    # Output arguments
    parser.add_argument("--output_dir", type=str, default=CONFIG["OUTPUT_DIR"])
    # Use BooleanOptionalAction for flags: --use-wandb or --no-use-wandb
    parser.add_argument("--use_wandb", action=argparse.BooleanOptionalAction, default=CONFIG["USE_WANDB"])

    # Projection arguments (ensure CONFIG are correctly sourced if PROJECTION_CONFIG was loaded)
    parser.add_argument("--projection_dim", type=int, default=CONFIG["PROJECTION_DIM"])
    parser.add_argument("--projection_layers", type=int, default=CONFIG["PROJECTION_LAYERS"])

    args = parser.parse_args()

    # --- Basic Checks ---
    if not os.path.isdir(args.data_dir):
        print(f"Error: Data directory not found at '{args.data_dir}'")
    if args.projection_model_path and not os.path.exists(args.projection_model_path):
         print(f"Warning: Specified projection model path '{args.projection_model_path}' does not exist.")
    # --------------------

    main(args)
